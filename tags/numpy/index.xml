<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>NumPy on Shih-Chin</title><link>https://shihchinw.github.io/tags/numpy.html</link><description>Recent content in NumPy on Shih-Chin</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><copyright>Copyright © Shih-Chin</copyright><lastBuildDate>Sun, 17 Mar 2019 12:10:34 +0800</lastBuildDate><atom:link href="https://shihchinw.github.io/tags/numpy/index.xml" rel="self" type="application/rss+xml"/><item><title>Performance Tips of NumPy ndarray</title><link>https://shihchinw.github.io/2019/03/performance-tips-of-numpy-ndarray.html</link><pubDate>Sun, 17 Mar 2019 12:10:34 +0800</pubDate><guid>https://shihchinw.github.io/2019/03/performance-tips-of-numpy-ndarray.html</guid><description>&lt;p>When I did homework assignments of the famous Deep Learning course &lt;a href="http://cs231n.stanford.edu/">CS231n&lt;/a> from Stanford, I was so impressed by &lt;strong>100X↑&lt;/strong> performance boost by using &lt;em>broadcasting&lt;/em> mechanism in NumPy. However, broadcasting doesn&amp;rsquo;t always speed up computation, we should also take into account &lt;em>memory usage&lt;/em> and &lt;em>memory access pattern&lt;/em> [&lt;a href="#ref.2">2&lt;/a>], or we will get a slower execution instead. This post shows several experiments and my reasoning of why certain operation is performant or not.&lt;/p></description></item></channel></rss>